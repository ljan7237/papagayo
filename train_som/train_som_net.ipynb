{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27129,
     "status": "ok",
     "timestamp": 1740382544372,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "Tr7PNKeV-uFC",
    "outputId": "e9a8d0d9-8872-43dd-e6ea-149b9268804c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54358,
     "status": "ok",
     "timestamp": 1740382598733,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "VlJexKL__irf",
    "outputId": "cf06c1cd-d692-472d-9e16-202f1389645b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpsom\n",
      "  Downloading simpsom-2.0.2-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from simpsom) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.11/dist-packages (from simpsom) (1.6.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.3 in /usr/local/lib/python3.11/dist-packages (from simpsom) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.3->simpsom) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->simpsom) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->simpsom) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->simpsom) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->simpsom) (1.17.0)\n",
      "Downloading simpsom-2.0.2-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: simpsom\n",
      "Successfully installed simpsom-2.0.2\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.0/548.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.7/849.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.9/196.9 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m188.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.2/841.2 kB\u001b[0m \u001b[31m183.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.3/513.3 kB\u001b[0m \u001b[31m174.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.0/916.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.5/244.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#install reqs\n",
    "! pip install simpsom\n",
    "!pip install -q cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1740015597409,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "_2aW8aPaAPEY",
    "outputId": "db14a6d2-41d4-4232-c617-40452e3572f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3.0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "print(cp.__version__)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16915,
     "status": "ok",
     "timestamp": 1740369607771,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "sv1dc4bL_iF3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import psutil\n",
    "import simpsom as sps\n",
    "\n",
    "\n",
    "#butcherbirds = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_butcherbird.npy\",mmap_mode='r').astype(np.float32)\n",
    "#superb_lyrebirds = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_superb_lyrebird.npy\").astype(np.float32)\n",
    "noisy_friarbirds = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_noisy_friarbird.npy\").astype(np.float32)\n",
    "#lorikeets = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_lorikeet.npy\").astype(np.float32)\n",
    "#kookaburras = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_kookaburra.npy\").astype(np.float32)\n",
    "#magpies = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_magpie.npy\").astype(np.float32)\n",
    "#australian_birds = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_australian_birds.npy\").astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2695414,
     "status": "ok",
     "timestamp": 1740372313253,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "zIBzvH3rjs1P",
    "outputId": "c5ef2492-9e78-4643-bf17-a66cf4bc21be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (200619, 2049)\n",
      "Data type: float32\n",
      "Memory required (GB): 1.644273324\n",
      "System RAM available (GB): 82.477756416\n",
      "Periodic Boundary Conditions active.\n",
      "The weights will be initialized with PCA.\n",
      "The map will be trained with the batch algorithm.\n",
      "Training SOM... done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import psutil\n",
    "import simpsom as sps\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#data = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset.npy\").astype(np.float32)\n",
    "data = noisy_friarbirds\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Data type:\", data.dtype)\n",
    "print(\"Memory required (GB):\", data.nbytes / 1e9)\n",
    "print(\"System RAM available (GB):\", psutil.virtual_memory().available / 1e9)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "net = sps.SOMNet(8, 8, data, PBC=True, random_seed=32, GPU=True)\n",
    "NUM_EPOCHS = 200 # -1 for \"until convergence\"\n",
    "net.train(train_algo='batch', start_learning_rate=0.01, epochs=NUM_EPOCHS, batch_size=4096)\n",
    "net.save(fileName=f\"noisy_friarbirds_8x8_som_net_{NUM_EPOCHS}_epochs\",out_path=\"/content/drive/MyDrive/papagayo/som_nets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8590971,
     "status": "ok",
     "timestamp": 1740391323155,
     "user": {
      "displayName": "Louis Janse van Rensburg",
      "userId": "17606134187402939115"
     },
     "user_tz": -600
    },
    "id": "EMgzIZKgXNOV",
    "outputId": "edc295a4-f454-4883-bccc-7f308cc59a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(661144, 8, 8) (661144, 2049)\n",
      "Using device: cuda\n",
      "Epoch [1/200], Loss: 2.251405759573964\n",
      "Validation Loss: 2.260652361659862\n",
      "Epoch [2/200], Loss: 2.2501306043226394\n",
      "Validation Loss: 2.260543645143026\n",
      "Epoch [3/200], Loss: 2.249995415677006\n",
      "Validation Loss: 2.260394511290549\n",
      "Epoch [4/200], Loss: 2.249839825430935\n",
      "Validation Loss: 2.260288011544976\n",
      "Epoch [5/200], Loss: 2.2497397402744412\n",
      "Validation Loss: 2.2598315046164132\n",
      "Epoch [6/200], Loss: 2.249464232532488\n",
      "Validation Loss: 2.26097104390157\n",
      "Epoch [7/200], Loss: 2.249844725399991\n",
      "Validation Loss: 2.260369177686874\n",
      "Epoch [8/200], Loss: 2.2498207342618173\n",
      "Validation Loss: 2.2603708871110944\n",
      "Epoch [9/200], Loss: 2.2498665499085164\n",
      "Validation Loss: 2.2603814412758365\n",
      "Epoch [10/200], Loss: 2.2498502868723325\n",
      "Validation Loss: 2.2603907295196937\n",
      "Epoch [11/200], Loss: 2.2498279950651416\n",
      "Validation Loss: 2.2603522276250296\n",
      "Epoch [12/200], Loss: 2.2498380201857726\n",
      "Validation Loss: 2.2603569581974887\n",
      "Epoch [13/200], Loss: 2.249987215972962\n",
      "Validation Loss: 2.2603681924760326\n",
      "Epoch [14/200], Loss: 2.2499290564559433\n",
      "Validation Loss: 2.2603731735956716\n",
      "Epoch [15/200], Loss: 2.2498465638506997\n",
      "Validation Loss: 2.2603570012370753\n",
      "Epoch [16/200], Loss: 2.250180933824868\n",
      "Validation Loss: 2.2603781145762767\n",
      "Epoch [17/200], Loss: 2.2498779257812305\n",
      "Validation Loss: 2.2603492810222563\n",
      "Epoch [18/200], Loss: 2.2498452327029876\n",
      "Validation Loss: 2.2603627924167182\n",
      "Epoch [19/200], Loss: 2.249838999733604\n",
      "Validation Loss: 2.2603661724547193\n",
      "Epoch [20/200], Loss: 2.249862645108313\n",
      "Validation Loss: 2.2603578428390123\n",
      "Epoch [21/200], Loss: 2.2498417341120085\n",
      "Validation Loss: 2.260370644126263\n",
      "Epoch [22/200], Loss: 2.2498250222622977\n",
      "Validation Loss: 2.260355852915665\n",
      "Epoch [23/200], Loss: 2.2499631770061987\n",
      "Validation Loss: 2.2603704149086914\n",
      "Epoch [24/200], Loss: 2.2498472513226093\n",
      "Validation Loss: 2.2603645065532105\n",
      "Epoch [25/200], Loss: 2.2498483248710834\n",
      "Validation Loss: 2.260376564329123\n",
      "Epoch [26/200], Loss: 2.2498440749366737\n",
      "Validation Loss: 2.260351161086946\n",
      "Epoch [27/200], Loss: 2.249817881762704\n",
      "Validation Loss: 2.2603901023567516\n",
      "Epoch [28/200], Loss: 2.2498383345503297\n",
      "Validation Loss: 2.260358472170609\n",
      "Epoch [29/200], Loss: 2.249842070119265\n",
      "Validation Loss: 2.260380658907187\n",
      "Epoch [30/200], Loss: 2.2499836025824598\n",
      "Validation Loss: 2.260369095775555\n",
      "Epoch [31/200], Loss: 2.2498256676216815\n",
      "Validation Loss: 2.2603930404976267\n",
      "Epoch [32/200], Loss: 2.2500217778816567\n",
      "Validation Loss: 2.260374603173345\n",
      "Epoch [33/200], Loss: 2.249850838832835\n",
      "Validation Loss: 2.260350611753998\n",
      "Epoch [34/200], Loss: 2.249813828382715\n",
      "Validation Loss: 2.260350925698714\n",
      "Epoch [35/200], Loss: 2.249838869148983\n",
      "Validation Loss: 2.2603588128674703\n",
      "Epoch [36/200], Loss: 2.2498283932544774\n",
      "Validation Loss: 2.260387962053539\n",
      "Epoch [37/200], Loss: 2.249951927523351\n",
      "Validation Loss: 2.260385051133513\n",
      "Epoch [38/200], Loss: 2.2498381371710217\n",
      "Validation Loss: 2.26036114196259\n",
      "Epoch [39/200], Loss: 2.249835919087336\n",
      "Validation Loss: 2.2603659772145352\n",
      "Epoch [40/200], Loss: 2.2498635919567995\n",
      "Validation Loss: 2.2603759314840537\n",
      "Epoch [41/200], Loss: 2.2498587136340484\n",
      "Validation Loss: 2.260439234295625\n",
      "Epoch [42/200], Loss: 2.249822668525667\n",
      "Validation Loss: 2.2603756413170193\n",
      "Epoch [43/200], Loss: 2.2498400507906013\n",
      "Validation Loss: 2.260354960962369\n",
      "Epoch [44/200], Loss: 2.249858207139712\n",
      "Validation Loss: 2.2603770042901434\n",
      "Epoch [45/200], Loss: 2.249847408391973\n",
      "Validation Loss: 2.2603698839705473\n",
      "Epoch [46/200], Loss: 2.249845398615324\n",
      "Validation Loss: 2.2603633979832525\n",
      "Epoch [47/200], Loss: 2.249821999898601\n",
      "Validation Loss: 2.260381473308105\n",
      "Epoch [48/200], Loss: 2.249829966000867\n",
      "Validation Loss: 2.260371876318533\n",
      "Epoch [49/200], Loss: 2.2498404582015463\n",
      "Validation Loss: 2.2603511770823492\n",
      "Epoch [50/200], Loss: 2.249844569323197\n",
      "Validation Loss: 2.260360315687253\n",
      "Epoch [51/200], Loss: 2.2498675113385764\n",
      "Validation Loss: 2.260353041522993\n",
      "Epoch [52/200], Loss: 2.2498816292719654\n",
      "Validation Loss: 2.260359220253608\n",
      "Epoch [53/200], Loss: 2.2498544437824415\n",
      "Validation Loss: 2.260367775822188\n",
      "Epoch [54/200], Loss: 2.249994943395209\n",
      "Validation Loss: 2.2603635866057057\n",
      "Epoch [55/200], Loss: 2.2498310126859615\n",
      "Validation Loss: 2.2603668822392464\n",
      "Epoch [56/200], Loss: 2.2498711450109394\n",
      "Validation Loss: 2.2603624963422186\n",
      "Epoch [57/200], Loss: 2.249828454192465\n",
      "Validation Loss: 2.2603924230999386\n",
      "Epoch [58/200], Loss: 2.2499604459183282\n",
      "Validation Loss: 2.260358762660328\n",
      "Epoch [59/200], Loss: 2.2498849108116517\n",
      "Validation Loss: 2.2603506235472954\n",
      "Epoch [60/200], Loss: 2.2498798247962473\n",
      "Validation Loss: 2.260351718661862\n",
      "Epoch [61/200], Loss: 2.249878584686796\n",
      "Validation Loss: 2.2604033511060795\n",
      "Epoch [62/200], Loss: 2.249826343935525\n",
      "Validation Loss: 2.260362930471488\n",
      "Epoch [63/200], Loss: 2.2498580707506792\n",
      "Validation Loss: 2.2603499049908047\n",
      "Epoch [64/200], Loss: 2.2498216424438233\n",
      "Validation Loss: 2.260390969223602\n",
      "Epoch [65/200], Loss: 2.249829866972468\n",
      "Validation Loss: 2.2603822250704253\n",
      "Epoch [66/200], Loss: 2.249850740742464\n",
      "Validation Loss: 2.260390878725998\n",
      "Epoch [67/200], Loss: 2.2498517829952545\n",
      "Validation Loss: 2.2603766383716337\n",
      "Epoch [68/200], Loss: 2.2498251015722834\n",
      "Validation Loss: 2.260377844662635\n",
      "Epoch [69/200], Loss: 2.249822116631876\n",
      "Validation Loss: 2.260370606024286\n",
      "Epoch [70/200], Loss: 2.2498518742845874\n",
      "Validation Loss: 2.260421466429355\n",
      "Epoch [71/200], Loss: 2.2498245790201405\n",
      "Validation Loss: 2.2603634742971717\n",
      "Epoch [72/200], Loss: 2.2498930648543514\n",
      "Validation Loss: 2.260399785699508\n",
      "Epoch [73/200], Loss: 2.2498209744541295\n",
      "Validation Loss: 2.260384337523645\n",
      "Epoch [74/200], Loss: 2.2498184468006004\n",
      "Validation Loss: 2.260397313797687\n",
      "Epoch [75/200], Loss: 2.2498537146974456\n",
      "Validation Loss: 2.2603559866259153\n",
      "Epoch [76/200], Loss: 2.2498316275578705\n",
      "Validation Loss: 2.2603914966518786\n",
      "Epoch [77/200], Loss: 2.2498391767783126\n",
      "Validation Loss: 2.26036151583103\n",
      "Epoch [78/200], Loss: 2.24984230877314\n",
      "Validation Loss: 2.260360840211372\n",
      "Epoch [79/200], Loss: 2.2498788540409036\n",
      "Validation Loss: 2.2603654380704774\n",
      "Epoch [80/200], Loss: 2.249817407343412\n",
      "Validation Loss: 2.260377174786396\n",
      "Epoch [81/200], Loss: 2.249832383658602\n",
      "Validation Loss: 2.2603566505802677\n",
      "Epoch [82/200], Loss: 2.2498218194303843\n",
      "Validation Loss: 2.2603571930953845\n",
      "Epoch [83/200], Loss: 2.2498557095210963\n",
      "Validation Loss: 2.2603657228942935\n",
      "Epoch [84/200], Loss: 2.2498484137732015\n",
      "Validation Loss: 2.2603501165977495\n",
      "Epoch [85/200], Loss: 2.24983742063423\n",
      "Validation Loss: 2.2603664618123394\n",
      "Epoch [86/200], Loss: 2.2498303488148443\n",
      "Validation Loss: 2.2603942159064854\n",
      "Epoch [87/200], Loss: 2.249936434362615\n",
      "Validation Loss: 2.2603529004865486\n",
      "Epoch [88/200], Loss: 2.249908089941567\n",
      "Validation Loss: 2.260400619569642\n",
      "Epoch [89/200], Loss: 2.249822315370671\n",
      "Validation Loss: 2.2603810597693346\n",
      "Epoch [90/200], Loss: 2.2498386028420003\n",
      "Validation Loss: 2.260385617732771\n",
      "Epoch [91/200], Loss: 2.249868131067404\n",
      "Validation Loss: 2.2603671650656656\n",
      "Epoch [92/200], Loss: 2.2498812109675494\n",
      "Validation Loss: 2.2603864165997782\n",
      "Epoch [93/200], Loss: 2.2500226102943817\n",
      "Validation Loss: 2.260388538818252\n",
      "Epoch [94/200], Loss: 2.2498758392697766\n",
      "Validation Loss: 2.260366108779566\n",
      "Epoch [95/200], Loss: 2.2499364653220337\n",
      "Validation Loss: 2.260373068443877\n",
      "Epoch [96/200], Loss: 2.249827421025155\n",
      "Validation Loss: 2.2603509270128863\n",
      "Epoch [97/200], Loss: 2.2498912959231627\n",
      "Validation Loss: 2.2603556822337336\n",
      "Epoch [98/200], Loss: 2.249833730078449\n",
      "Validation Loss: 2.2603650463516507\n",
      "Epoch [99/200], Loss: 2.2498692439395636\n",
      "Validation Loss: 2.2603494511417432\n",
      "Epoch [100/200], Loss: 2.249875041331481\n",
      "Validation Loss: 2.2603901119381296\n",
      "Epoch [101/200], Loss: 2.2499023204790722\n",
      "Validation Loss: 2.26041030593193\n",
      "Epoch [102/200], Loss: 2.2498600343800663\n",
      "Validation Loss: 2.2603506382285263\n",
      "Epoch [103/200], Loss: 2.249827842898223\n",
      "Validation Loss: 2.260369125500361\n",
      "Epoch [104/200], Loss: 2.2498356522651366\n",
      "Validation Loss: 2.2603565142182385\n",
      "Epoch [105/200], Loss: 2.249873734028873\n",
      "Validation Loss: 2.260373418842898\n",
      "Epoch [106/200], Loss: 2.24982530642471\n",
      "Validation Loss: 2.2603871919090337\n",
      "Epoch [107/200], Loss: 2.2498859205359345\n",
      "Validation Loss: 2.2603739137017005\n",
      "Epoch [108/200], Loss: 2.249889350909634\n",
      "Validation Loss: 2.2603613362491433\n",
      "Epoch [109/200], Loss: 2.2498234952209764\n",
      "Validation Loss: 2.2603801424285415\n",
      "Epoch [110/200], Loss: 2.249839631798328\n",
      "Validation Loss: 2.2603697712924586\n",
      "Epoch [111/200], Loss: 2.2498355538822237\n",
      "Validation Loss: 2.2603572912274448\n",
      "Epoch [112/200], Loss: 2.2498286786758572\n",
      "Validation Loss: 2.260366676343272\n",
      "Epoch [113/200], Loss: 2.2498460423312436\n",
      "Validation Loss: 2.260394944493228\n",
      "Epoch [114/200], Loss: 2.2498352407027093\n",
      "Validation Loss: 2.2603632345085365\n",
      "Epoch [115/200], Loss: 2.249818334187705\n",
      "Validation Loss: 2.260381068607998\n",
      "Epoch [116/200], Loss: 2.2498901365864072\n",
      "Validation Loss: 2.260368628157431\n",
      "Epoch [117/200], Loss: 2.2499405194958078\n",
      "Validation Loss: 2.260369281165758\n",
      "Epoch [118/200], Loss: 2.2498294657022004\n",
      "Validation Loss: 2.260383597489725\n",
      "Epoch [119/200], Loss: 2.249844664503474\n",
      "Validation Loss: 2.2603965413258895\n",
      "Epoch [120/200], Loss: 2.249829372816733\n",
      "Validation Loss: 2.260365520991311\n",
      "Epoch [121/200], Loss: 2.2499357016269297\n",
      "Validation Loss: 2.2603842986320832\n",
      "Epoch [122/200], Loss: 2.2498206282426114\n",
      "Validation Loss: 2.260408171826418\n",
      "Epoch [123/200], Loss: 2.249825412194196\n",
      "Validation Loss: 2.2603752029045223\n",
      "Epoch [124/200], Loss: 2.24988565099341\n",
      "Validation Loss: 2.260356774465775\n",
      "Epoch [125/200], Loss: 2.2498227952049197\n",
      "Validation Loss: 2.2603743490117414\n",
      "Epoch [126/200], Loss: 2.2498841209811427\n",
      "Validation Loss: 2.2603647442020343\n",
      "Epoch [127/200], Loss: 2.2498450294656274\n",
      "Validation Loss: 2.2603842455640475\n",
      "Epoch [128/200], Loss: 2.249872167170286\n",
      "Validation Loss: 2.260390026911736\n",
      "Epoch [129/200], Loss: 2.2498695647057874\n",
      "Validation Loss: 2.26037991420426\n",
      "Epoch [130/200], Loss: 2.2498172446887046\n",
      "Validation Loss: 2.2603686195747517\n",
      "Epoch [131/200], Loss: 2.2501317303632464\n",
      "Validation Loss: 2.2603576118890407\n",
      "Epoch [132/200], Loss: 2.249906914087683\n",
      "Validation Loss: 2.260374364516809\n",
      "Epoch [133/200], Loss: 2.2499639481514326\n",
      "Validation Loss: 2.2603584543959356\n",
      "Epoch [134/200], Loss: 2.2498669413238597\n",
      "Validation Loss: 2.26037409972285\n",
      "Epoch [135/200], Loss: 2.24983404823884\n",
      "Validation Loss: 2.260369566553821\n",
      "Epoch [136/200], Loss: 2.249843182490182\n",
      "Validation Loss: 2.2603740121528393\n",
      "Epoch [137/200], Loss: 2.2498367106428963\n",
      "Validation Loss: 2.260379230521102\n",
      "Epoch [138/200], Loss: 2.2499544269540137\n",
      "Validation Loss: 2.2603671471557893\n",
      "Epoch [139/200], Loss: 2.2499080284730377\n",
      "Validation Loss: 2.2603640627704547\n",
      "Epoch [140/200], Loss: 2.2498582174724397\n",
      "Validation Loss: 2.260375080008701\n",
      "Epoch [141/200], Loss: 2.249843324897737\n",
      "Validation Loss: 2.260362255787434\n",
      "Epoch [142/200], Loss: 2.2498450855759953\n",
      "Validation Loss: 2.2603644091764834\n",
      "Epoch [143/200], Loss: 2.249889826377389\n",
      "Validation Loss: 2.260390811027207\n",
      "Epoch [144/200], Loss: 2.249843199454463\n",
      "Validation Loss: 2.2603920612501343\n",
      "Epoch [145/200], Loss: 2.2500060086599842\n",
      "Validation Loss: 2.2603851637989822\n",
      "Epoch [146/200], Loss: 2.249827044100332\n",
      "Validation Loss: 2.260376125109014\n",
      "Epoch [147/200], Loss: 2.2499097228898557\n",
      "Validation Loss: 2.26036178535709\n",
      "Epoch [148/200], Loss: 2.2498287111971527\n",
      "Validation Loss: 2.26035615699057\n",
      "Epoch [149/200], Loss: 2.249825494393992\n",
      "Validation Loss: 2.2603663016221525\n",
      "Epoch [150/200], Loss: 2.249835590898028\n",
      "Validation Loss: 2.2603649424130765\n",
      "Epoch [151/200], Loss: 2.2498519206171106\n",
      "Validation Loss: 2.2603866558926695\n",
      "Epoch [152/200], Loss: 2.2498667119299083\n",
      "Validation Loss: 2.260353816889935\n",
      "Epoch [153/200], Loss: 2.2498379242680553\n",
      "Validation Loss: 2.260368001620028\n",
      "Epoch [154/200], Loss: 2.249811782551278\n",
      "Validation Loss: 2.2603588747345102\n",
      "Epoch [155/200], Loss: 2.249823627600967\n",
      "Validation Loss: 2.2603802284500283\n",
      "Epoch [156/200], Loss: 2.249822421564771\n",
      "Validation Loss: 2.260370814590067\n",
      "Epoch [157/200], Loss: 2.249810548411667\n",
      "Validation Loss: 2.260370850454887\n",
      "Epoch [158/200], Loss: 2.2498254973910825\n",
      "Validation Loss: 2.2603620716501114\n",
      "Epoch [159/200], Loss: 2.2498295616704023\n",
      "Validation Loss: 2.260366157953759\n",
      "Epoch [160/200], Loss: 2.24983758503878\n",
      "Validation Loss: 2.260378186873769\n",
      "Epoch [161/200], Loss: 2.249818716236746\n",
      "Validation Loss: 2.2603670485541243\n",
      "Epoch [162/200], Loss: 2.249867699839302\n",
      "Validation Loss: 2.260367518951118\n",
      "Epoch [163/200], Loss: 2.2498397127427627\n",
      "Validation Loss: 2.26035386086152\n",
      "Epoch [164/200], Loss: 2.249826491488484\n",
      "Validation Loss: 2.260349729921089\n",
      "Epoch [165/200], Loss: 2.2498527917553655\n",
      "Validation Loss: 2.2604023006895884\n",
      "Epoch [166/200], Loss: 2.2500864688222078\n",
      "Validation Loss: 2.260368635229444\n",
      "Epoch [167/200], Loss: 2.2499006219436777\n",
      "Validation Loss: 2.2603588742658065\n",
      "Epoch [168/200], Loss: 2.2498723038766526\n",
      "Validation Loss: 2.2603514489050553\n",
      "Epoch [169/200], Loss: 2.249832324831966\n",
      "Validation Loss: 2.260372668645326\n",
      "Epoch [170/200], Loss: 2.25043155139658\n",
      "Validation Loss: 2.2603680386908573\n",
      "Epoch [171/200], Loss: 2.2498229693260114\n",
      "Validation Loss: 2.2603827597113257\n",
      "Epoch [172/200], Loss: 2.249829294267627\n",
      "Validation Loss: 2.2603590614172524\n",
      "Epoch [173/200], Loss: 2.2498191284297837\n",
      "Validation Loss: 2.2603840494928162\n",
      "Epoch [174/200], Loss: 2.24987062218744\n",
      "Validation Loss: 2.260369739689234\n",
      "Epoch [175/200], Loss: 2.250003864276149\n",
      "Validation Loss: 2.2603616219382583\n",
      "Epoch [176/200], Loss: 2.249811097406181\n",
      "Validation Loss: 2.2604116161056216\n",
      "Epoch [177/200], Loss: 2.2500020393774824\n",
      "Validation Loss: 2.260354268572145\n",
      "Epoch [178/200], Loss: 2.2500668806792157\n",
      "Validation Loss: 2.260367289315319\n",
      "Epoch [179/200], Loss: 2.249812806372569\n",
      "Validation Loss: 2.2604110914408917\n",
      "Epoch [180/200], Loss: 2.249953201614553\n",
      "Validation Loss: 2.2603586167818173\n",
      "Epoch [181/200], Loss: 2.249926966142796\n",
      "Validation Loss: 2.260362811303661\n",
      "Epoch [182/200], Loss: 2.249860844284186\n",
      "Validation Loss: 2.26038115382007\n",
      "Epoch [183/200], Loss: 2.2499367865318036\n",
      "Validation Loss: 2.2603615629807847\n",
      "Epoch [184/200], Loss: 2.249860954913746\n",
      "Validation Loss: 2.260367051684522\n",
      "Epoch [185/200], Loss: 2.2499099451519124\n",
      "Validation Loss: 2.2603628788456165\n",
      "Epoch [186/200], Loss: 2.2498291220667648\n",
      "Validation Loss: 2.2603835520047686\n",
      "Epoch [187/200], Loss: 2.2498644545238893\n",
      "Validation Loss: 2.2603609445176978\n",
      "Epoch [188/200], Loss: 2.249892383175134\n",
      "Validation Loss: 2.2603465114595553\n",
      "Epoch [189/200], Loss: 2.249837104819584\n",
      "Validation Loss: 2.260374877536063\n",
      "Epoch [190/200], Loss: 2.2498886308316868\n",
      "Validation Loss: 2.2603729192501922\n",
      "Epoch [191/200], Loss: 2.249830248934062\n",
      "Validation Loss: 2.2603856760629033\n",
      "Epoch [192/200], Loss: 2.249924243214548\n",
      "Validation Loss: 2.2603536171754364\n",
      "Epoch [193/200], Loss: 2.2498834053855337\n",
      "Validation Loss: 2.2603598727175176\n",
      "Epoch [194/200], Loss: 2.249827639423313\n",
      "Validation Loss: 2.260386951783293\n",
      "Epoch [195/200], Loss: 2.2498391809392606\n",
      "Validation Loss: 2.2604092353846297\n",
      "Epoch [196/200], Loss: 2.2498324886920003\n",
      "Validation Loss: 2.2603818576718946\n",
      "Epoch [197/200], Loss: 2.24984259757791\n",
      "Validation Loss: 2.260357006089958\n",
      "Epoch [198/200], Loss: 2.2499493922898735\n",
      "Validation Loss: 2.2603581383943325\n",
      "Epoch [199/200], Loss: 2.249855818814609\n",
      "Validation Loss: 2.2603667940094456\n",
      "Epoch [200/200], Loss: 2.2498264653161164\n",
      "Validation Loss: 2.260406621060085\n"
     ]
    }
   ],
   "source": [
    "# For training an upsampling CNN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the data from the .npy files\n",
    "A = np.load(\"/content/drive/MyDrive/papagayo/upsampling/butcherbird_embeddings.npy\").astype(np.float32)  # Shape (N, 8, 8)\n",
    "B = np.load(\"/content/drive/MyDrive/papagayo/training_data/final_dataset_butcherbird.npy\").astype(np.float32)  # Shape (N, 2049)\n",
    "\n",
    "print(A.shape, B.shape)\n",
    "# Convert to torch tensors\n",
    "X = torch.tensor(A, dtype=torch.float32).unsqueeze(1)  # Adding channel dimension (N, 1, 8, 8)\n",
    "y = torch.tensor(B, dtype=torch.float32)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for batching during training\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a CNN model\n",
    "class PredictionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredictionCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # (1, 8, 8) -> (16, 8, 8)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) # (16, 8, 8) -> (32, 8, 8)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)  # 32 channels * 8 * 8 grid\n",
    "        self.fc2 = nn.Linear(256, 2049)  # Output size is 2049\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions with ReLU activations\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "\n",
    "        # Flatten the output of convolution layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten (N, 32, 8, 8) -> (N, 32*8*8)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN model and move it to the device (GPU or CPU)\n",
    "model = PredictionCNN().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training progress\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/papagayo/upsampling/prediction_cnn_model_australian_birds.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT81181tEksr"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPiolyuDQCu40BnFbQ/9+1K",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
